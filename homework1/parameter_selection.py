# -*- coding: utf-8 -*-
"""
Created on Tue Apr  4 18:57:57 2023

@author: HONOR
"""

import model as my
import numpy as np
import time

seed = 123
batch_size = 256
min_epochs = 5  # 最小训练epoch数
max_epochs = 30  # 最大训练epoch数
cut_off_n = 4  # 若有连续cut_off_n个epoch的accuracy都持续减小，则停止训练

# 候选参数集合
hiddens = [128, 256, 512, 1024]
lr_decays = [0.99, 0.98, 0.9, 0.8]
lrs = [1e-1, 1e-2, 1e-3, 1e-4]
reg_lambdas = [0, 1e-1, 1e-2, 1e-3]

#  检验是否单调减
def monotonicDecrease(x):
    dx = np.diff(x)
    return np.all(dx <= 0)


if __name__ == '__main__':

    data, label = my.loadMinist(onehot_needed=False)
    # 只用训练集的60000条数据，划分为train和val
    split_n = int(data.shape[0] * 0.8)
    train_data, train_label = data[0:split_n], label[0:split_n]
    test_data, test_label = data[split_n:], label[split_n:]

    result = {}
    for hidden in hiddens:
        for lr_decay in lr_decays:
            for lr in lrs:
                for reg_lambda in reg_lambdas:
                    #  对于超参数的组合进行训练
                    mynet = my.TwoLayerNet(784, hidden, 10, std=1e-4)
                    mynet.lr = lr
                    mynet.l2_lambda = reg_lambda

                    train_dataloader = my.MyDataLoader(train_data, my.oneHot(train_label, 10), batch_size=batch_size,
                                                       drop_last=True)

                    test_accuracy_list = []
                    for e in range(max_epochs):
                        for batch_data, batch_label in train_dataloader:
                            loss = mynet.train(batch_data, batch_label, L2reg=True, lambd=reg_lambda)
                        # 每个epoch训练后令学习率衰减
                        mynet.lr = mynet.lr * lr_decay
                        # 计算 accuracy
                        test_result, _ = mynet.predict(test_data, my.oneHot(test_label, 10))
                        count_test = my.test(test_result, test_label)
                        test_accuracy = count_test / test_result.shape[0]

                        test_accuracy_list.append(test_accuracy)
                        if e > min_epochs:
                            if test_accuracy_list[-1] < 0.7:
                                # 训练了10个epoch效果不好就结束
                                break
                            if monotonicDecrease(test_accuracy_list[-cut_off_n:]):
                                # 最近的n个周期在持续下降就结束
                                break

                    result[(hidden, lr_decay, lr, reg_lambda)] = np.mean(test_accuracy_list[-cut_off_n:])

                    print('end_time:', time.asctime())
                    print('end epoch:', e + 1)
                    print((hidden, lr_decay, lr, reg_lambda), ' ', result[(hidden, lr_decay, lr, reg_lambda)])


  #   result = {(128, 0.99, 0.1, 0): 0.106,
  # (128, 0.99, 0.1, 0.1): 0.106,
  # (128, 0.99, 0.1, 0.01): 0.106,
  # (128, 0.99, 0.1, 0.001): 0.106,
  # (128, 0.99, 0.01, 0): 0.106,
  # (128, 0.99, 0.01, 0.1): 0.21335416666666668,
  # (128, 0.99, 0.01, 0.01): 0.2129375,
  # (128, 0.99, 0.01, 0.001): 0.106,
  # (128, 0.99, 0.001, 0): 0.9735416666666667,
  # (128, 0.99, 0.001, 0.1): 0.9740833333333334,
  # (128, 0.99, 0.001, 0.01): 0.9744583333333333,
  # (128, 0.99, 0.001, 0.001): 0.9735208333333334,
  # (128, 0.99, 0.0001, 0): 0.9263541666666666,
  # (128, 0.99, 0.0001, 0.1): 0.9244791666666667,
  # (128, 0.99, 0.0001, 0.01): 0.9264375,
  # (128, 0.99, 0.0001, 0.001): 0.9245,
  # (128, 0.98, 0.1, 0): 0.106,
  # (128, 0.98, 0.1, 0.1): 0.106,
  # (128, 0.98, 0.1, 0.01): 0.106,
  # (128, 0.98, 0.1, 0.001): 0.106,
  # (128, 0.98, 0.01, 0): 0.2119375,
  # (128, 0.98, 0.01, 0.1): 0.21122916666666666,
  # (128, 0.98, 0.01, 0.01): 0.2116875,
  # (128, 0.98, 0.01, 0.001): 0.106,
  # (128, 0.98, 0.001, 0): 0.974,
  # (128, 0.98, 0.001, 0.1): 0.9736458333333333,
  # (128, 0.98, 0.001, 0.01): 0.9736041666666667,
  # (128, 0.98, 0.001, 0.001): 0.9733750000000001,
  # (128, 0.98, 0.0001, 0): 0.9221874999999999,
  # (128, 0.98, 0.0001, 0.1): 0.921875,
  # (128, 0.98, 0.0001, 0.01): 0.9225833333333333,
  # (128, 0.98, 0.0001, 0.001): 0.9218333333333334,
  # (128, 0.9, 0.1, 0): 0.106,
  # (128, 0.9, 0.1, 0.1): 0.106,
  # (128, 0.9, 0.1, 0.01): 0.106,
  # (128, 0.9, 0.1, 0.001): 0.106,
  # (128, 0.9, 0.01, 0): 0.106,
  # (128, 0.9, 0.01, 0.1): 0.306625,
  # (128, 0.9, 0.01, 0.01): 0.2175625,
  # (128, 0.9, 0.01, 0.001): 0.106,
  # (128, 0.9, 0.001, 0): 0.9636041666666667,
  # (128, 0.9, 0.001, 0.1): 0.9628541666666666,
  # (128, 0.9, 0.001, 0.01): 0.9643125,
  # (128, 0.9, 0.001, 0.001): 0.9623958333333335,
  # (128, 0.9, 0.0001, 0): 0.8969583333333333,
  # (128, 0.9, 0.0001, 0.1): 0.8965833333333333,
  # (128, 0.9, 0.0001, 0.01): 0.896625,
  # (128, 0.9, 0.0001, 0.001): 0.8965833333333333,
  # (128, 0.8, 0.1, 0): 0.106,
  # (128, 0.8, 0.1, 0.1): 0.106,
  # (128, 0.8, 0.1, 0.01): 0.106,
  # (128, 0.8, 0.1, 0.001): 0.106,
  # (128, 0.8, 0.01, 0): 0.17783333333333334,
  # (128, 0.8, 0.01, 0.1): 0.1815,
  # (128, 0.8, 0.01, 0.01): 0.1947708333333333,
  # (128, 0.8, 0.01, 0.001): 0.38845833333333335,
  # (128, 0.8, 0.001, 0): 0.9504166666666667,
  # (128, 0.8, 0.001, 0.1): 0.9500833333333334,
  # (128, 0.8, 0.001, 0.01): 0.9503541666666666,
  # (128, 0.8, 0.001, 0.001): 0.9499166666666666,
  # (128, 0.8, 0.0001, 0): 0.8532708333333334,
  # (128, 0.8, 0.0001, 0.1): 0.8565833333333334,
  # (128, 0.8, 0.0001, 0.01): 0.8546666666666667,
  # (128, 0.8, 0.0001, 0.001): 0.8551249999999999,
  # (256, 0.99, 0.1, 0): 0.106,
  # (256, 0.99, 0.1, 0.1): 0.106,
  # (256, 0.99, 0.1, 0.01): 0.10641666666666667,
  # (256, 0.99, 0.1, 0.001): 0.106,
  # (256, 0.99, 0.01, 0): 0.106,
  # (256, 0.99, 0.01, 0.1): 0.2612916666666667,
  # (256, 0.99, 0.01, 0.01): 0.2135,
  # (256, 0.99, 0.01, 0.001): 0.15820833333333334,
  # (256, 0.99, 0.001, 0): 0.9753125,
  # (256, 0.99, 0.001, 0.1): 0.9764791666666668,
  # (256, 0.99, 0.001, 0.01): 0.9764583333333333,
  # (256, 0.99, 0.001, 0.001): 0.9760625,
  # (256, 0.99, 0.0001, 0): 0.9266041666666667,
  # (256, 0.99, 0.0001, 0.1): 0.9265833333333333,
  # (256, 0.99, 0.0001, 0.01): 0.9262916666666666,
  # (256, 0.99, 0.0001, 0.001): 0.9260625,
  # (256, 0.98, 0.1, 0): 0.106,
  # (256, 0.98, 0.1, 0.1): 0.106,
  # (256, 0.98, 0.1, 0.01): 0.106,
  # (256, 0.98, 0.1, 0.001): 0.106,
  # (256, 0.98, 0.01, 0): 0.21258333333333332,
  # (256, 0.98, 0.01, 0.1): 0.106,
  # (256, 0.98, 0.01, 0.01): 0.3011666666666667,
  # (256, 0.98, 0.01, 0.001): 0.21087499999999998,
  # (256, 0.98, 0.001, 0): 0.9755625,
  # (256, 0.98, 0.001, 0.1): 0.9761249999999999,
  # (256, 0.98, 0.001, 0.01): 0.9752916666666667,
  # (256, 0.98, 0.001, 0.001): 0.9758749999999999,
  # (256, 0.98, 0.0001, 0): 0.9222083333333334,
  # (256, 0.98, 0.0001, 0.1): 0.9230625,
  # (256, 0.98, 0.0001, 0.01): 0.9233541666666667,
  # (256, 0.98, 0.0001, 0.001): 0.9227708333333333,
  # (256, 0.9, 0.1, 0): 0.106,
  # (256, 0.9, 0.1, 0.1): 0.106,
  # (256, 0.9, 0.1, 0.01): 0.106,
  # (256, 0.9, 0.1, 0.001): 0.106,
  # (256, 0.9, 0.01, 0): 0.1299375,
  # (256, 0.9, 0.01, 0.1): 0.36383333333333334,
  # (256, 0.9, 0.01, 0.01): 0.32066666666666666,
  # (256, 0.9, 0.01, 0.001): 0.33587500000000003,
  # (256, 0.9, 0.001, 0): 0.9653333333333334,
  # (256, 0.9, 0.001, 0.1): 0.9667291666666666,
  # (256, 0.9, 0.001, 0.01): 0.9646250000000001,
  # (256, 0.9, 0.001, 0.001): 0.9656666666666667,
  # (256, 0.9, 0.0001, 0): 0.8989583333333333,
  # (256, 0.9, 0.0001, 0.1): 0.8992916666666667,
  # (256, 0.9, 0.0001, 0.01): 0.8995208333333333,
  # (256, 0.9, 0.0001, 0.001): 0.898875,
  # (256, 0.8, 0.1, 0): 0.106,
  # (256, 0.8, 0.1, 0.1): 0.106,
  # (256, 0.8, 0.1, 0.01): 0.106,
  # (256, 0.8, 0.1, 0.001): 0.10591666666666667,
  # (256, 0.8, 0.01, 0): 0.3474583333333333,
  # (256, 0.8, 0.01, 0.1): 0.21208333333333335,
  # (256, 0.8, 0.01, 0.01): 0.32779166666666665,
  # (256, 0.8, 0.01, 0.001): 0.20447916666666668,
  # (256, 0.8, 0.001, 0): 0.9531666666666667,
  # (256, 0.8, 0.001, 0.1): 0.9528333333333333,
  # (256, 0.8, 0.001, 0.01): 0.9525833333333333,
  # (256, 0.8, 0.001, 0.001): 0.9527291666666666,
  # (256, 0.8, 0.0001, 0): 0.8613958333333334,
  # (256, 0.8, 0.0001, 0.1): 0.8617291666666667,
  # (256, 0.8, 0.0001, 0.01): 0.8626875,
  # (256, 0.8, 0.0001, 0.001): 0.8617083333333333,
  # (512, 0.99, 0.1, 0): 0.10608333333333334,
  # (512, 0.99, 0.1, 0.1): 0.106,
  # (512, 0.99, 0.1, 0.01): 0.106,
  # (512, 0.99, 0.1, 0.001): 0.106,
  # (512, 0.99, 0.01, 0): 0.23277083333333334,
  # (512, 0.99, 0.01, 0.1): 0.30389583333333337,
  # (512, 0.99, 0.01, 0.01): 0.23112500000000002,
  # (512, 0.99, 0.01, 0.001): 0.21333333333333332,
  # (512, 0.99, 0.001, 0): 0.9775833333333332,
  # (512, 0.99, 0.001, 0.1): 0.9770000000000001,
  # (512, 0.99, 0.001, 0.01): 0.9775208333333334,
  # (512, 0.99, 0.001, 0.001): 0.9767083333333334,
  # (512, 0.99, 0.0001, 0): 0.9288958333333333,
  # (512, 0.99, 0.0001, 0.1): 0.9291875,
  # (512, 0.99, 0.0001, 0.01): 0.9283958333333333,
  # (512, 0.99, 0.0001, 0.001): 0.9279375,
  # (512, 0.98, 0.1, 0): 0.10616666666666667,
  # (512, 0.98, 0.1, 0.1): 0.106,
  # (512, 0.98, 0.1, 0.01): 0.109,
  # (512, 0.98, 0.1, 0.001): 0.106,
  # (512, 0.98, 0.01, 0): 0.106,
  # (512, 0.98, 0.01, 0.1): 0.2139375,
  # (512, 0.98, 0.01, 0.01): 0.208625,
  # (512, 0.98, 0.01, 0.001): 0.21304166666666666,
  # (512, 0.98, 0.001, 0): 0.9765416666666666,
  # (512, 0.98, 0.001, 0.1): 0.9763541666666667,
  # (512, 0.98, 0.001, 0.01): 0.9761458333333333,
  # (512, 0.98, 0.001, 0.001): 0.9765416666666666,
  # (512, 0.98, 0.0001, 0): 0.9250625,
  # (512, 0.98, 0.0001, 0.1): 0.9245625,
  # (512, 0.98, 0.0001, 0.01): 0.9241041666666667,
  # (512, 0.98, 0.0001, 0.001): 0.9257083333333334,
  # (512, 0.9, 0.1, 0): 0.106,
  # (512, 0.9, 0.1, 0.1): 0.106,
  # (512, 0.9, 0.1, 0.01): 0.10608333333333334,
  # (512, 0.9, 0.1, 0.001): 0.106,
  # (512, 0.9, 0.01, 0): 0.1570625,
  # (512, 0.9, 0.01, 0.1): 0.23404166666666668,
  # (512, 0.9, 0.01, 0.01): 0.24085416666666665,
  # (512, 0.9, 0.01, 0.001): 0.106,
  # (512, 0.9, 0.001, 0): 0.9673750000000001,
  # (512, 0.9, 0.001, 0.1): 0.9671458333333334,
  # (512, 0.9, 0.001, 0.01): 0.9664583333333334,
  # (512, 0.9, 0.001, 0.001): 0.9658958333333334,
  # (512, 0.9, 0.0001, 0): 0.9016041666666667,
  # (512, 0.9, 0.0001, 0.1): 0.9015,
  # (512, 0.9, 0.0001, 0.01): 0.9015208333333333,
  # (512, 0.9, 0.0001, 0.001): 0.9016875,
  # (512, 0.8, 0.1, 0): 0.106,
  # (512, 0.8, 0.1, 0.1): 0.10608333333333334,
  # (512, 0.8, 0.1, 0.01): 0.106,
  # (512, 0.8, 0.1, 0.001): 0.106,
  # (512, 0.8, 0.01, 0): 0.37835416666666666,
  # (512, 0.8, 0.01, 0.1): 0.25825,
  # (512, 0.8, 0.01, 0.01): 0.20185416666666667,
  # (512, 0.8, 0.01, 0.001): 0.3995208333333333,
  # (512, 0.8, 0.001, 0): 0.9551666666666667,
  # (512, 0.8, 0.001, 0.1): 0.9533333333333334,
  # (512, 0.8, 0.001, 0.01): 0.9535833333333333,
  # (512, 0.8, 0.001, 0.001): 0.9538541666666667,
  # (512, 0.8, 0.0001, 0): 0.8694166666666667,
  # (512, 0.8, 0.0001, 0.1): 0.8690208333333334,
  # (512, 0.8, 0.0001, 0.01): 0.8671666666666666,
  # (512, 0.8, 0.0001, 0.001): 0.8691666666666666,
  # (1024, 0.99, 0.1, 0): 0.10625,
  # (1024, 0.99, 0.1, 0.1): 0.10616666666666667,
  # (1024, 0.99, 0.1, 0.01): 0.106,
  # (1024, 0.99, 0.1, 0.001): 0.106,
  # (1024, 0.99, 0.01, 0): 0.222125,
  # (1024, 0.99, 0.01, 0.1): 0.20795833333333333,
  # (1024, 0.99, 0.01, 0.01): 0.21077083333333332,
  # (1024, 0.99, 0.01, 0.001): 0.2094583333333333,
  # (1024, 0.99, 0.001, 0): 0.9770833333333333,
  # (1024, 0.99, 0.001, 0.1): 0.9777083333333333,
  # (1024, 0.99, 0.001, 0.01): 0.978125,
  # (1024, 0.99, 0.001, 0.001): 0.9773333333333334,
  # (1024, 0.99, 0.0001, 0): 0.9281875,
  # (1024, 0.99, 0.0001, 0.1): 0.9299583333333334,
  # (1024, 0.99, 0.0001, 0.01): 0.9297708333333333,
  # (1024, 0.99, 0.0001, 0.001): 0.9297916666666666,
  # (1024, 0.98, 0.1, 0): 0.106,
  # (1024, 0.98, 0.1, 0.1): 0.10616666666666667,
  # (1024, 0.98, 0.1, 0.01): 0.10591666666666667,
  # (1024, 0.98, 0.1, 0.001): 0.106,
  # (1024, 0.98, 0.01, 0): 0.11074999999999999,
  # (1024, 0.98, 0.01, 0.1): 0.18191666666666667,
  # (1024, 0.98, 0.01, 0.01): 0.10602083333333333,
  # (1024, 0.98, 0.01, 0.001): 0.20847916666666666,
  # (1024, 0.98, 0.001, 0): 0.9766666666666666,
  # (1024, 0.98, 0.001, 0.1): 0.9772708333333332,
  # (1024, 0.98, 0.001, 0.01): 0.9776666666666667,
  # (1024, 0.98, 0.001, 0.001): 0.97675,
  # (1024, 0.98, 0.0001, 0): 0.9254583333333334,
  # (1024, 0.98, 0.0001, 0.1): 0.925,
  # (1024, 0.98, 0.0001, 0.01): 0.9256875000000001,
  # (1024, 0.98, 0.0001, 0.001): 0.9253958333333333,
  # (1024, 0.9, 0.1, 0): 0.106,
  # (1024, 0.9, 0.1, 0.1): 0.10691666666666666,
  # (1024, 0.9, 0.1, 0.01): 0.106,
  # (1024, 0.9, 0.1, 0.001): 0.106,
  # (1024, 0.9, 0.01, 0): 0.20914583333333334,
  # (1024, 0.9, 0.01, 0.1): 0.13075,
  # (1024, 0.9, 0.01, 0.01): 0.2788333333333334,
  # (1024, 0.9, 0.01, 0.001): 0.28347916666666667,
  # (1024, 0.9, 0.001, 0): 0.9679166666666668,
  # (1024, 0.9, 0.001, 0.1): 0.9669166666666666,
  # (1024, 0.9, 0.001, 0.01): 0.9676666666666667,
  # (1024, 0.9, 0.001, 0.001): 0.9676458333333334,
  # (1024, 0.9, 0.0001, 0): 0.90375,
  # (1024, 0.9, 0.0001, 0.1): 0.9029375000000001,
  # (1024, 0.9, 0.0001, 0.01): 0.9029166666666666,
  # (1024, 0.9, 0.0001, 0.001): 0.9037083333333333,
  # (1024, 0.8, 0.1, 0): 0.10608333333333334,
  # (1024, 0.8, 0.1, 0.1): 0.10775,
  # (1024, 0.8, 0.1, 0.01): 0.106,
  # (1024, 0.8, 0.1, 0.001): 0.106,
  # (1024, 0.8, 0.01, 0): 0.8484375,
  # (1024, 0.8, 0.01, 0.1): 0.2936666666666667,
  # (1024, 0.8, 0.01, 0.01): 0.33722916666666664,
  # (1024, 0.8, 0.01, 0.001): 0.3128958333333334,
  # (1024, 0.8, 0.001, 0): 0.9546666666666667,
  # (1024, 0.8, 0.001, 0.1): 0.9542083333333333,
  # (1024, 0.8, 0.001, 0.01): 0.9530416666666667,
  # (1024, 0.8, 0.001, 0.001): 0.954625,
  # (1024, 0.8, 0.0001, 0): 0.8744583333333333,
  # (1024, 0.8, 0.0001, 0.1): 0.8751458333333333,
  # (1024, 0.8, 0.0001, 0.01): 0.875125,
  # (1024, 0.8, 0.0001, 0.001): 0.8751249999999999}

    # hiddens = [128, 256, 512, 1024]
    # lr_decays = [0.99, 0.98, 0.9, 0.8]
    # lrs = [1e-1, 1e-2, 1e-3, 1e-4]
    # reg_lambdas = [0, 1e-1, 1e-2, 1e-3]
    
    print(result)

    good_result = {}

    count_hiddens = [0] * 4
    count_lr_decays = [0] * 4
    count_lrs = [0] * 4
    count_reg_lambdas = [0] * 4
    count_good_result = 0

    for key, value in result.items():
        if value >= 0.970:
            good_result[key] = value
            print(key, value)

            count_hiddens[hiddens.index(key[0])] += 1
            count_lr_decays[lr_decays.index(key[1])] += 1
            count_lrs[lrs.index(key[2])] += 1
            count_reg_lambdas[reg_lambdas.index(key[3])] += 1

            count_good_result += 1

# 共32个good result
# hiddens: 8 8 8 8
# lr_decays: 16 16 0 0
# lrs: 0 0 32 0
# reg_lambdas: 8 8 8 8
# 这个说明不了啥，我们可以将门槛(0.97)提高,然后看比例变化


# (128, 0.99, 0.001, 0) 0.9735416666666667
# (128, 0.99, 0.001, 0.1) 0.9740833333333334
# (128, 0.99, 0.001, 0.01) 0.9744583333333333
# (128, 0.99, 0.001, 0.001) 0.9735208333333334
# (128, 0.98, 0.001, 0) 0.974
# (128, 0.98, 0.001, 0.1) 0.9736458333333333
# (128, 0.98, 0.001, 0.01) 0.9736041666666667
# (128, 0.98, 0.001, 0.001) 0.9733750000000001
# (256, 0.99, 0.001, 0) 0.9753125
# (256, 0.99, 0.001, 0.1) 0.9764791666666668
# (256, 0.99, 0.001, 0.01) 0.9764583333333333
# (256, 0.99, 0.001, 0.001) 0.9760625
# (256, 0.98, 0.001, 0) 0.9755625
# (256, 0.98, 0.001, 0.1) 0.9761249999999999
# (256, 0.98, 0.001, 0.01) 0.9752916666666667
# (256, 0.98, 0.001, 0.001) 0.9758749999999999
# (512, 0.99, 0.001, 0) 0.9775833333333332
# (512, 0.99, 0.001, 0.1) 0.9770000000000001
# (512, 0.99, 0.001, 0.01) 0.9775208333333334
# (512, 0.99, 0.001, 0.001) 0.9767083333333334
# (512, 0.98, 0.001, 0) 0.9765416666666666
# (512, 0.98, 0.001, 0.1) 0.9763541666666667
# (512, 0.98, 0.001, 0.01) 0.9761458333333333
# (512, 0.98, 0.001, 0.001) 0.9765416666666666
# (1024, 0.99, 0.001, 0) 0.9770833333333333
# (1024, 0.99, 0.001, 0.1) 0.9777083333333333
# (1024, 0.99, 0.001, 0.01) 0.978125
# (1024, 0.99, 0.001, 0.001) 0.9773333333333334
# (1024, 0.98, 0.001, 0) 0.9766666666666666
# (1024, 0.98, 0.001, 0.1) 0.9772708333333332
# (1024, 0.98, 0.001, 0.01) 0.9776666666666667
# (1024, 0.98, 0.001, 0.001) 0.97675
